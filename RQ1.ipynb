{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1: Which technologies have been investigated in the last decade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datene einlesen\n",
    "#data = pd.read_csv('./data/data.csv')\n",
    "#data.drop([\"Unnamed: 0\"], axis = 1, inplace = True)\n",
    "\n",
    "data = pd.read_csv('./data/export1.csv',dtype={'Abstract': str}, usecols = ['Document Title','Abstract','Publication Year'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTermDocMatrix(column):\n",
    "        \n",
    "    count_vectorizer = CountVectorizer(stop_words='english',ngram_range = (1,2),dtype=np.int32)\n",
    "    sparse_matrix = count_vectorizer.fit_transform(data[column].values.astype('U'))\n",
    "\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix, columns=count_vectorizer.get_feature_names(), dtype=np.int32)\n",
    "    df['PublishingYear'] = data['Publication Year']\n",
    "    returndf = df.groupby('PublishingYear').sum().transpose().astype('int32')\n",
    "    return returndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowWord(data,word):\n",
    "    d = data.loc[word]\n",
    "    d.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaltenEntf(data):\n",
    "    y = []\n",
    "    for item in data.columns:\n",
    "        y = data.columns\n",
    "\n",
    "\n",
    "    for index in range(len(y)):\n",
    "        x = y[index]\n",
    "        if x in stopwords or x.isnumeric():\n",
    "            del data[y[index]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Untersuchung Abstract\n",
    "#abstract = CreateTermDocMatrix('Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = abstract.loc[abstract[2011] > 20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b.plot.bar()\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#cnt = Counter()\n",
    "\n",
    "#for abstract in data['Abstract']:\n",
    "#    if type(abstract) is str:\n",
    "#        wordlist = abstract.split(' ')\n",
    "#        for word in wordlist:\n",
    "#            cnt[word] += 1\n",
    "        \n",
    "#cnt.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for abstract in data.itertuples():\n",
    "#    print(abstract[1])\n",
    "#    print(abstract[2])\n",
    "#    print(abstract[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data.csv',usecols = ['Document Title','Abstract','Publication Year'], nrows =1000)\n",
    "data.columns = ['title','abstract','year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Untersuchung Title\n",
    "#title = CreateTermDocMatrix('title')\n",
    "\n",
    "#print(title)\n",
    "#title = SpaltenEntf(title)\n",
    "#print(title)\n",
    "\n",
    "#b = title.loc[title[2017] > 5] \n",
    "\n",
    "#b.plot.bar()\n",
    "#print(b)\n",
    "#b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#from spacy.symbols import NOUN\n",
    "#from collections import Counter\n",
    "\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#cnt = Counter()\n",
    "#tokenlist = []\n",
    "#i = 1\n",
    "\n",
    "#for columns in data.itertuples():\n",
    "\n",
    "#    text = columns[1] #1 = title 2 = year 3 = abstract\n",
    "    #print(type(abstract))\n",
    "    # Verarbeite den Text\n",
    "#    if type(text) is str:\n",
    "#        doc = nlp(text)\n",
    "\n",
    "#        for noun_phrase in list(doc.noun_chunks):\n",
    "#             noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_)\n",
    "\n",
    "#        [(token.text,token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "years = []\n",
    "\n",
    "#Balkendiagramm\n",
    "#plt.bar(range(len(years)),anzah)\n",
    "\n",
    "#Titel\n",
    "#plt.title(\"Toller Plot\")\n",
    "#plt.ylabel(\"# of awards\")\n",
    "\n",
    "#plt.xticks(range(len(years)),years)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = [{'POS': 'ADJ', 'OP': '?'},\n",
    " #          {'OP': '+', 'POS': 'NOUN'},\n",
    "  #         {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "#doc = nlp(\"I work on artificial intelligence topics.\")\n",
    "\n",
    "#matcher.add(\"match_nouns\", [pattern])\n",
    "#matches = matcher(doc)\n",
    "#sp  = [doc[match[1]:match[2]] for match in matches] \n",
    "\n",
    "#filtered = filter_spans(sp)\n",
    "#with doc.retokenize() as retokenizer:\n",
    "#    for s in filtered:\n",
    "#        retokenizer.merge(s, attrs = {\"POS\": \"NOUN\"})\n",
    "#[(token.pos_, token.text) for token in doc]\n",
    "\n",
    "#doc[3].vector == (nlp.vocab[\"artificial\"].vector + \n",
    "#                  nlp.vocab[\"intelligence\"].vector + \n",
    "#                  nlp.vocab[\"topics\"].vector)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import NOUN\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "words = Counter()\n",
    "words_chunk = Counter()\n",
    "tokenlist = []\n",
    "chunklist = []\n",
    "i = 1\n",
    "\n",
    "for columns in data.itertuples():\n",
    "\n",
    "    text = columns[3] #1 = title 2 = year 3 = abstract\n",
    "    #print(type(abstract))\n",
    "    # Verarbeite den Text\n",
    "    if type(text) is str:\n",
    "        doc = nlp(text)\n",
    "        #list(dosc.noun_chunks)\n",
    "        #print(i)\n",
    "        i+= 1\n",
    "        \n",
    "\n",
    "        \n",
    "        for token in doc:\n",
    "            # Greife auf den Text, die Wortart und die Dependenzrelation des Tokens zu\n",
    "            token_text = token.text\n",
    "            token_pos = token.pos_\n",
    "            token_dep = token.dep_\n",
    "            token_ent = token.ent_type_\n",
    "            \n",
    "            if token.pos == NOUN:\n",
    "                if not token.is_stop:\n",
    "                    #print(f\"{token_text}: {token_ent}\")\n",
    "                    #print(token.tag_)\n",
    "                    words[token_text] += 1\n",
    "                    #tokenlist.append([token_text,columns[2],token_ent])\n",
    "                    \n",
    "        for chunk in doc.noun_chunks:\n",
    "            words_chunk[chunk.text] += 1\n",
    "\n",
    "\n",
    "#tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_chunk.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
